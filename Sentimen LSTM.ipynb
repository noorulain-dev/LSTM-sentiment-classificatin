{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qC5_lrEi0nw"
      },
      "source": [
        "\n",
        "## **Name: Noor Ul Ain Khurshid**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYLvC2YqJRS"
      },
      "source": [
        "\n",
        "\n",
        "There are several forms of LSTM that can be used. At the very basic level a variation of using LSTM would be stack LSTM layers together. A sample code is given below on how it can be easily done using keras.\n",
        "\n",
        "Now adapting either from  jupyter notebook 7b or 7c to stack 2, 3 or more layers togethers. Using the same datasets given in the notebook. Including scripts here with observations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EY_xRV1i3q7",
        "outputId": "4078e666-a9af-48fa-cde1-a618597bbd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pugnlp in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: coverage in /usr/local/lib/python3.10/dist-packages (from pugnlp) (7.2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pugnlp) (0.18.3)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (from pugnlp) (0.18.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.10/dist-packages (from pugnlp) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pugnlp) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pugnlp) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pugnlp) (1.5.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from pugnlp) (23.1.2)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from pugnlp) (1.11)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (from pugnlp) (0.21.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from pugnlp) (8.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pugnlp) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pugnlp) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from pugnlp) (5.13.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from pugnlp) (0.12.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pugnlp) (4.65.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from pugnlp) (0.40.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pugnlp) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim->pugnlp) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pugnlp) (6.3.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (6.4.8)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (5.4.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (5.5.6)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter->pugnlp) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pugnlp) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pugnlp) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pugnlp) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pugnlp) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pugnlp) (2022.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->pugnlp) (8.2.2)\n",
            "Requirement already satisfied: Levenshtein==0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein->pugnlp) (0.21.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.21.0->python-Levenshtein->pugnlp) (3.0.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->pugnlp) (1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pugnlp) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pugnlp) (1.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->pugnlp) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->pugnlp) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->pugnlp) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->pugnlp) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter->pugnlp) (6.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->pugnlp) (3.6.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter->pugnlp) (3.0.7)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->pugnlp) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter->pugnlp) (2.14.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (3.1.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (5.3.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (0.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (2.1.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (0.7.4)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (5.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter->pugnlp) (1.2.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (21.3.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (1.5.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter->pugnlp) (0.16.0)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter->pugnlp) (2.3.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (4.8.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter->pugnlp) (3.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->pugnlp) (2.16.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert->jupyter->pugnlp) (4.3.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->pugnlp) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter->pugnlp) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter->pugnlp) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter->pugnlp) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->pugnlp) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->pugnlp) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->pugnlp) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->pugnlp) (2.21)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pugnlp/constants.py:137: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  DATE_TYPES = (datetime.datetime, datetime.date)\n",
            "/usr/local/lib/python3.10/dist-packages/pugnlp/constants.py:159: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  ZERO_TIMESTAMP = pd.Timestamp('1970-01-01 00:00:00', tz='utc')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "\n",
        "import requests\n",
        "!pip install pugnlp\n",
        "from pugnlp.futil import path_status, find_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gn8JZ2Zrq1AJ"
      },
      "outputs": [],
      "source": [
        "# From the nlpia package for downloading data too big for the repo\n",
        "import tqdm\n",
        "BIG_URLS = {\n",
        "    'w2v': (\n",
        "        'https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1',\n",
        "        1647046227,\n",
        "    ),\n",
        "    'slang': (\n",
        "        'https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1',\n",
        "        117633024,\n",
        "    ),\n",
        "    'tweets': (\n",
        "        'https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1',\n",
        "        311725313,\n",
        "    ),\n",
        "    'lsa_tweets': (\n",
        "        'https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1',\n",
        "        3112841563,  # 3112841312,\n",
        "    ),\n",
        "    'imdb': (\n",
        "        'https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1',\n",
        "        3112841563,  # 3112841312,\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "InOmnd7xq-NX"
      },
      "outputs": [],
      "source": [
        "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
        "def dropbox_basename(url):\n",
        "    filename = os.path.basename(url)\n",
        "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
        "    if match:\n",
        "        return filename[:-len(match[0])]\n",
        "    return filename\n",
        "\n",
        "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
        "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
        "    if filename is None:\n",
        "        filename = dropbox_basename(url)\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    if url.endswith('?dl=0'):\n",
        "        url = url[:-1] + '1'  # noninteractive download\n",
        "    if verbose:\n",
        "        tqdm_prog = tqdm\n",
        "        print('requesting URL: {}'.format(url))\n",
        "    else:\n",
        "        tqdm_prog = no_tqdm\n",
        "    r = requests.get(url, stream=True, allow_redirects=True)\n",
        "    size = r.headers.get('Content-Length', None) if size is None else size\n",
        "    print('remote size: {}'.format(size))\n",
        "\n",
        "    stat = path_status(file_path)\n",
        "    print('local size: {}'.format(stat.get('size', None)))\n",
        "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
        "        r.close()\n",
        "        return file_path\n",
        "\n",
        "    print('Downloading to {}'.format(file_path))\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "    r.close()\n",
        "    return file_path\n",
        "\n",
        "def untar(fname):\n",
        "    if fname.endswith(\"tar.gz\"):\n",
        "        with tarfile.open(fname) as tf:\n",
        "            tf.extractall()\n",
        "    else:\n",
        "        print(\"Not a tar.gz file: {}\".format(fname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyiut1nfq-mX",
        "outputId": "47dddcd2-d19e-4b81-9dbd-2a4181f5fa85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "remote size: 1647046227\n",
            "local size: None\n",
            "Downloading to ./GoogleNews-vectors-negative300.bin.gz\n",
            "requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n",
            "remote size: 84125825\n",
            "local size: None\n",
            "Downloading to ./aclImdb_v1.tar.gz\n"
          ]
        }
      ],
      "source": [
        "#  UNCOMMENT these 2 lines if you haven't already download the word2vec model and the imdb dataset\n",
        "download_file(BIG_URLS['w2v'][0])\n",
        "untar(download_file(BIG_URLS['imdb'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLlcU_ALroHc"
      },
      "source": [
        "But what is the memory? The memory is going to be represented by a vector that is the\n",
        "same number of elements as neurons in the cell. Your example has a relatively simple\n",
        "50 neurons, so the memory unit will be a vector of floats that is 50 elements long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BrJt5w0uq-rx"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "    \"\"\"\n",
        "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "    \"\"\"\n",
        "    positive_path = os.path.join(filepath, 'pos')\n",
        "    negative_path = os.path.join(filepath, 'neg')\n",
        "    \n",
        "    pos_label = 1\n",
        "    neg_label = 0\n",
        "    \n",
        "    dataset = []\n",
        "    \n",
        "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((pos_label, f.read()))\n",
        "            \n",
        "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "        with open(filename, 'r') as f:\n",
        "            dataset.append((neg_label, f.read()))\n",
        "    \n",
        "    shuffle(dataset)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tyQl4QQrq-un"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "\n",
        "            except KeyError:\n",
        "                pass  # No matching token in the Google w2v vocab\n",
        "            \n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XHodLdKFrxwp"
      },
      "outputs": [],
      "source": [
        "def collect_expected(dataset):\n",
        "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
        "    expected = []\n",
        "    for sample in dataset:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cnFKehIprzJB"
      },
      "outputs": [],
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    # Create a vector of 0's the length of our word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        " \n",
        "        if len(sample) > maxlen:\n",
        "            temp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            temp = sample\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7LlQXdKrr04R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dataset = pre_process_data('./aclImdb/train')\n",
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "\n",
        "split_point = int(len(vectorized_data)*.2)\n",
        "split_point2 = int(len(vectorized_data)*.25)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:split_point2]\n",
        "y_test = expected[split_point:split_point2]\n",
        "\n",
        "maxlen = 400\n",
        "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
        "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
        "epochs = 20\n",
        "\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IljKTUyksTWh"
      },
      "source": [
        "# **Building LSTM Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOH_jhirWk6"
      },
      "source": [
        "One import and one line of Keras code changed. But a great deal more is going on\n",
        "under the surface. From the summary, you can see you have many more parameters to\n",
        "train than you did in the SimpleRNN from last chapter for the same number of neurons\n",
        "(50). Recall the simple RNN had the following weights:\n",
        "\n",
        "*   300 (one for each element of the input vector)\n",
        "*   1 (one for the bias term)\n",
        "*   50 (one for each neuronâ€™s output from the previous time step)\n",
        "\n",
        "For a total of 351 per neuron.\n",
        "\n",
        "351 * 50 = 17,550 for the layer\n",
        "\n",
        "The cells have three gates (a total of four neurons):\n",
        "\n",
        "17,550 * 4 = 70,200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBB-4ILIr255",
        "outputId": "4b5d3fbe-ea0f-4db4-b7fd-0b1a17588a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 400, 50)           70200     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 400, 50)           20200     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 50)                20200     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50)                0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 110,651\n",
            "Trainable params: 110,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "\n",
        "num_neurons = 50\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(LSTM(num_neurons, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
        "model.add(LSTM(num_neurons))\n",
        "model.add(Dropout(.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAP48bNFs83G"
      },
      "source": [
        "# **Training LSTM Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "secoJ9Aks3Xn",
        "outputId": "0d311e09-3daf-4ff6-bae0-638c937f8911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - 125s 760ms/step - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6952 - val_accuracy: 0.4928\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 117s 746ms/step - loss: 0.6907 - accuracy: 0.5192 - val_loss: 0.6897 - val_accuracy: 0.5264\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 119s 756ms/step - loss: 0.6881 - accuracy: 0.5112 - val_loss: 0.6928 - val_accuracy: 0.5280\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 116s 740ms/step - loss: 0.6849 - accuracy: 0.5210 - val_loss: 0.6920 - val_accuracy: 0.5296\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 123s 785ms/step - loss: 0.6812 - accuracy: 0.5312 - val_loss: 0.6795 - val_accuracy: 0.5512\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 115s 733ms/step - loss: 0.6829 - accuracy: 0.5360 - val_loss: 0.6797 - val_accuracy: 0.5440\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 115s 733ms/step - loss: 0.6652 - accuracy: 0.5810 - val_loss: 0.6676 - val_accuracy: 0.6224\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 114s 725ms/step - loss: 0.6668 - accuracy: 0.5892 - val_loss: 0.6861 - val_accuracy: 0.5120\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 117s 748ms/step - loss: 0.6573 - accuracy: 0.6164 - val_loss: 0.7070 - val_accuracy: 0.5232\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 115s 731ms/step - loss: 0.6511 - accuracy: 0.6188 - val_loss: 0.6417 - val_accuracy: 0.6632\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 118s 752ms/step - loss: 0.6475 - accuracy: 0.6282 - val_loss: 0.6723 - val_accuracy: 0.5704\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 115s 736ms/step - loss: 0.6198 - accuracy: 0.6828 - val_loss: 0.6397 - val_accuracy: 0.6352\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 119s 759ms/step - loss: 0.6323 - accuracy: 0.6532 - val_loss: 0.6070 - val_accuracy: 0.7112\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 118s 753ms/step - loss: 0.6169 - accuracy: 0.6902 - val_loss: 0.6284 - val_accuracy: 0.6528\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 116s 737ms/step - loss: 0.6453 - accuracy: 0.6136 - val_loss: 0.6240 - val_accuracy: 0.7120\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 115s 734ms/step - loss: 0.6125 - accuracy: 0.6946 - val_loss: 0.5550 - val_accuracy: 0.7568\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 120s 765ms/step - loss: 0.6745 - accuracy: 0.5636 - val_loss: 0.6053 - val_accuracy: 0.6816\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 118s 750ms/step - loss: 0.5917 - accuracy: 0.7192 - val_loss: 0.5753 - val_accuracy: 0.7376\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 124s 792ms/step - loss: 0.6242 - accuracy: 0.6644 - val_loss: 0.6721 - val_accuracy: 0.5856\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 116s 740ms/step - loss: 0.5884 - accuracy: 0.7240 - val_loss: 0.5561 - val_accuracy: 0.7560\n",
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))\n",
        "model_structure = model.to_json()\n",
        "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_structure)\n",
        "\n",
        "model.save_weights(\"lstm_weights1.h5\")\n",
        "print('Model saved.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2wEDCqF_s4zg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMzn7yEN3DMw"
      },
      "source": [
        "# **Saving and LSTM Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "frQV9aUk3F38"
      },
      "outputs": [],
      "source": [
        "model_structure = model.to_json()\n",
        "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights(\"lstm_weights1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zkHGESlc3cF3"
      },
      "outputs": [],
      "source": [
        "from keras.models import model_from_json\n",
        "with open(\"lstm_model1.json\", \"r\") as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('lstm_weights1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozkzLGTW3nr9"
      },
      "source": [
        "# **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-fxQbaM3hCI",
        "outputId": "7ec5cfee-3680-40b3-e35a-4d432575ff2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Sample's sentiment, 1 - pos, 2 - neg : [[0.26347402]]\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "Raw output of sigmoid function: [[0.26347402]]\n"
          ]
        }
      ],
      "source": [
        "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n",
        "\n",
        "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "\n",
        "print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict(test_vec)))\n",
        "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVz9OvkFzuS1"
      },
      "source": [
        "**Time and Performance Analysis**\n",
        "When 2-3 layers are stacked in an LSTM (Long Short-Term Memory) network, the time and performance of the network can be affected in several ways:\n",
        "\n",
        "Increased training time: Stacking LSTM layers increases the number of parameters that need to be learned during training. This can result in longer training times, especially for large datasets.\n",
        "\n",
        "Improved performance: Adding more layers to an LSTM network can improve its ability to learn complex patterns in the data. This can result in better performance, especially on tasks that require long-term memory and complex sequences."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
